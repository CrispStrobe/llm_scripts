{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-11T22:05:29.795540Z","iopub.status.busy":"2024-05-11T22:05:29.795016Z","iopub.status.idle":"2024-05-11T22:05:29.803596Z","shell.execute_reply":"2024-05-11T22:05:29.802197Z","shell.execute_reply.started":"2024-05-11T22:05:29.795487Z"},"trusted":true},"outputs":[],"source":["model_name='phi-2-spaetzle-v4' # model name that we download\n","repo_orig='cstr'  # account of the original repo from where we download the unquantized model\n","username = 'cstr' # account where we upload the quantized model to\n","fix_pretokenizer = True # must we use the pretokenizer fix with the update-script?\n","#only for the use of the convert.py script:\n","vocab_type = \"bpe\" # \"bpe\" for llama3 eg, spm\n","pad_vocab = True\n","quantization = \"Q4_K_M\" # good compromise is often \"Q4_K_M\"\n","model_type = \"phi2\" #will be used in bpe pretokenizer setup \"llama-bpe\" for llama3!\n","save_space = False\n","print (\"parameters initialized.\")"]},{"cell_type":"markdown","metadata":{},"source":["These are the quantization types we can use:\n","```\n","   2  or  Q4_0    :  3.56G, +0.2166 ppl @ LLaMA-v1-7B\n","   3  or  Q4_1    :  3.90G, +0.1585 ppl @ LLaMA-v1-7B\n","   8  or  Q5_0    :  4.33G, +0.0683 ppl @ LLaMA-v1-7B\n","   9  or  Q5_1    :  4.70G, +0.0349 ppl @ LLaMA-v1-7B\n","  19  or  IQ2_XXS :  2.06 bpw quantization\n","  20  or  IQ2_XS  :  2.31 bpw quantization\n","  10  or  Q2_K    :  2.63G, +0.6717 ppl @ LLaMA-v1-7B\n","  21  or  Q2_K_S  :  2.16G, +9.0634 ppl @ LLaMA-v1-7B\n","  23  or  IQ3_XXS :  3.06 bpw quantization\n","  12  or  Q3_K    : alias for Q3_K_M\n","  22  or  Q3_K_XS : 3-bit extra small quantization\n","  11  or  Q3_K_S  :  2.75G, +0.5551 ppl @ LLaMA-v1-7B\n","  12  or  Q3_K_M  :  3.07G, +0.2496 ppl @ LLaMA-v1-7B\n","  13  or  Q3_K_L  :  3.35G, +0.1764 ppl @ LLaMA-v1-7B\n","  15  or  Q4_K    : alias for Q4_K_M\n","  14  or  Q4_K_S  :  3.59G, +0.0992 ppl @ LLaMA-v1-7B\n","  15  or  Q4_K_M  :  3.80G, +0.0532 ppl @ LLaMA-v1-7B\n","  17  or  Q5_K    : alias for Q5_K_M\n","  16  or  Q5_K_S  :  4.33G, +0.0400 ppl @ LLaMA-v1-7B\n","  17  or  Q5_K_M  :  4.45G, +0.0122 ppl @ LLaMA-v1-7B\n","  18  or  Q6_K    :  5.15G, +0.0008 ppl @ LLaMA-v1-7B\n","   7  or  Q8_0    :  6.70G, +0.0004 ppl @ LLaMA-v1-7B\n","   1  or  F16     : 13.00G              @ 7B\n","   0  or  F32     : 26.00G              @ 7B\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-05-11T19:56:24.108405Z","iopub.status.busy":"2024-05-11T19:56:24.107912Z","iopub.status.idle":"2024-05-11T20:01:11.627010Z","shell.execute_reply":"2024-05-11T20:01:11.625765Z","shell.execute_reply.started":"2024-05-11T19:56:24.108360Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["!git clone https://github.com/ggerganov/llama.cpp\n","%cd 'llama.cpp'\n","!make"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-05-11T20:01:11.629465Z","iopub.status.busy":"2024-05-11T20:01:11.628862Z","iopub.status.idle":"2024-05-11T20:02:24.263184Z","shell.execute_reply":"2024-05-11T20:02:24.261738Z","shell.execute_reply.started":"2024-05-11T20:01:11.629428Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["!pip install huggingface_hub\n","\n","token = 'HF_TOKEN' # you must set the token in Add-ons/Secrets and attach it to this notebook\n","\n","from kaggle_secrets import UserSecretsClient\n","user_secrets = UserSecretsClient()\n","token_value = user_secrets.get_secret(\"HF_TOKEN\")\n","\n","from huggingface_hub import snapshot_download\n","outpath = snapshot_download(repo_id=repo_orig+\"/\"+model_name, token=token_value)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T20:14:11.672580Z","iopub.status.busy":"2024-05-11T20:14:11.672021Z","iopub.status.idle":"2024-05-11T20:14:11.680459Z","shell.execute_reply":"2024-05-11T20:14:11.679015Z","shell.execute_reply.started":"2024-05-11T20:14:11.672534Z"},"trusted":true},"outputs":[],"source":["print(\"downloaded to: \", outpath)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T21:23:09.959157Z","iopub.status.busy":"2024-05-11T21:23:09.958714Z","iopub.status.idle":"2024-05-11T21:23:10.455303Z","shell.execute_reply":"2024-05-11T21:23:10.453900Z","shell.execute_reply.started":"2024-05-11T21:23:09.959112Z"},"trusted":true},"outputs":[],"source":["import subprocess\n","\n","if not fix_pretokenizer and not model_type == \"phi2\":\n","    # Start command with basic parameters\n","    quantize_command = f\"python ./convert.py {outpath}/ --outtype f16 --outfile /kaggle/model/{model_name}.bin\"\n","    \n"," #   if model_type == \"phi2\":\n"," #        quantize_command = f\"python ./convert-hf-to-gguf.py {outpath}/ --outfile /kaggle/model/{model_name}.bin --outtype f32\"\n","    \n","    # Add vocab-type parameter if applicable\n","    if vocab_type != \"\":\n","        quantize_command += f\" --vocab-type {vocab_type}\"  \n","    \n","    # Add pad-vocab parameter if true\n","    if pad_vocab:\n","        quantize_command += \" --pad-vocab\"\n","    \n","    # Execute the command\n","    result = subprocess.run(quantize_command, shell=True, text=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n","\n","    # Check if the command was successful\n","    if result.returncode == 0:\n","        print(\"Command executed successfully.\")\n","        print(\"Output:\", result.stdout)\n","    else:\n","        print(\"Error in command execution.\")\n","        print(\"Error:\", result.stderr)\n","\n","#!python ./convert.py {outpath}/ --outtype f16 --outfile /kaggle/model/{model_name}.bin --vocab-type bpe --pad-vocab\n","#!python ./convert.py {outpath}/ --outtype f16 --outfile /kaggle/model/{model_name}.bin --pad-vocab"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T22:05:56.374120Z","iopub.status.busy":"2024-05-11T22:05:56.373647Z","iopub.status.idle":"2024-05-11T22:05:57.940599Z","shell.execute_reply":"2024-05-11T22:05:57.938669Z","shell.execute_reply.started":"2024-05-11T22:05:56.374085Z"},"trusted":true},"outputs":[],"source":["# in case we must revert:\n","#!wget \"https://raw.githubusercontent.com/ggerganov/llama.cpp/master/convert-hf-to-gguf-update.py\" -O convert-hf-to-gguf-update.py"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T22:06:02.675325Z","iopub.status.busy":"2024-05-11T22:06:02.674835Z","iopub.status.idle":"2024-05-11T22:06:02.690159Z","shell.execute_reply":"2024-05-11T22:06:02.688424Z","shell.execute_reply.started":"2024-05-11T22:06:02.675287Z"},"trusted":true},"outputs":[],"source":["import subprocess\n","\n","if fix_pretokenizer:\n","    # Read and modify the existing script\n","    with open(\"convert-hf-to-gguf-update.py\", \"r\") as file:\n","        script_contents = file.read()\n","\n","    new_repo_name = repo_orig + \"/\" + model_name # from which repo originates the model which we newly add to the list \n","    \n","    # Define the new line to be inserted\n","    new_line = f'    {{\"name\": \"{model_type}\", \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/{new_repo_name}\", }},'\n","    marker_line = '{\"name\": \"llama-bpe\",      \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/meta-llama/Meta-Llama-3-8B\", },'\n","    # Insert the new line after the marker line\n","    marker_found = False\n","    lines = script_contents.split('\\n')\n","    for i, line in enumerate(lines):\n","        if marker_line in line:\n","            lines.insert(i + 1, new_line)\n","            marker_found = True\n","            break\n","\n","    if not marker_found:\n","        print(\"Marker line not found in the file.\")\n","\n","    else:\n","        updated_script = \"\\n\".join(lines)\n","        # Write the updated script to the file\n","        with open(\"convert-hf-to-gguf-update.py\", \"w\") as file:\n","            file.write(updated_script)\n","        print(\"Script updated successfully.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T22:07:35.128333Z","iopub.status.busy":"2024-05-11T22:07:35.127350Z","iopub.status.idle":"2024-05-11T22:07:57.518111Z","shell.execute_reply":"2024-05-11T22:07:57.516742Z","shell.execute_reply.started":"2024-05-11T22:07:35.128272Z"},"trusted":true},"outputs":[],"source":["if fix_pretokenizer:\n","    # Execute the script and capture output\n","    command = f\"python convert-hf-to-gguf-update.py {token_value} > output.txt\"\n","    subprocess.run(command, shell=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T22:08:00.605996Z","iopub.status.busy":"2024-05-11T22:08:00.605484Z","iopub.status.idle":"2024-05-11T22:08:00.626174Z","shell.execute_reply":"2024-05-11T22:08:00.624524Z","shell.execute_reply.started":"2024-05-11T22:08:00.605957Z"},"trusted":true},"outputs":[],"source":["if fix_pretokenizer:\n","    # Read the output from the file and trim it after a specific line\n","    end_line = \"return res\"\n","    with open('output.txt', 'r') as file:\n","        lines = file.readlines()\n","        trimmed_output = []\n","        remainder_output = []\n","        capture_remainder = False\n","        for line in lines:\n","            if capture_remainder:\n","                remainder_output.append(line)  # Save the rest of lines after the specified line\n","            else:\n","                trimmed_output.append(line)\n","                if end_line in line:\n","                    capture_remainder = True  # Start capturing the remainder after this line\n","\n","    # Join the trimmed lines back into a single string\n","    script_output = \"\".join(trimmed_output)\n","    remainder_script_output = \"\".join(remainder_output)\n","    print(\"Captured Output:\", script_output)\n","    print(\"Remainder Output:\", remainder_script_output)  # Optionally print or process remainder output\n","\n","    # Function replacement from update script output\n","    def replace_function(source_path, output_path, start_marker, end_marker, new_function_content):\n","        with open(source_path, 'r') as source_file, open(output_path, 'w') as output_file:\n","            in_old_function = False\n","            for line in source_file:\n","                if start_marker in line:\n","                    in_old_function = True\n","                    output_file.write(new_function_content + \"\\n\")\n","                    continue\n","                if in_old_function and end_marker in line:\n","                    in_old_function = False\n","                    continue\n","                if not in_old_function:\n","                    output_file.write(line)\n","\n","    # Setup of the function replacement\n","    start_marker = \"def get_vocab_base_pre(self, tokenizer) -> str:\"\n","    end_marker = \"return res\"\n","    new_function_content = script_output  # Use the trimmed script output as the new content\n","\n","    source_path = \"convert-hf-to-gguf.py\"\n","    output_path = \"convert-hf-to-gguf-updated.py\"\n","    replace_function(source_path, output_path, start_marker, end_marker, new_function_content)\n","    print(\"Function replacement complete.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T22:08:12.951828Z","iopub.status.busy":"2024-05-11T22:08:12.951273Z","iopub.status.idle":"2024-05-11T22:09:30.760071Z","shell.execute_reply":"2024-05-11T22:09:30.758234Z","shell.execute_reply.started":"2024-05-11T22:08:12.951791Z"},"trusted":true},"outputs":[],"source":["if fix_pretokenizer:\n","    # Assume remainder_script_output contains the commands, one per line\n","    commands = remainder_script_output.strip().split('\\n')\n","\n","    for command in commands:\n","        if command.strip():  # Ensure the command is not just whitespace\n","            try:\n","                # Execute the command\n","                print(f\"Executing: {command}\")\n","                result = subprocess.run(command, shell=True, text=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n","                \n","                # Check if the command was successful\n","                if result.returncode == 0:\n","                    print(\"Command executed successfully.\")\n","                    print(\"Output:\", result.stdout)\n","                else:\n","                    print(\"Error in command execution.\")\n","                    print(\"Error:\", result.stderr)\n","            except Exception as e:\n","                print(f\"An error occurred while executing {command}: {str(e)}\")"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T20:02:43.976707Z","iopub.status.busy":"2024-05-11T20:02:43.976279Z","iopub.status.idle":"2024-05-11T20:02:45.136370Z","shell.execute_reply":"2024-05-11T20:02:45.130940Z","shell.execute_reply.started":"2024-05-11T20:02:43.976672Z"},"trusted":true},"outputs":[],"source":["!mkdir /kaggle/model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T22:13:34.167601Z","iopub.status.busy":"2024-05-11T22:13:34.166201Z","iopub.status.idle":"2024-05-11T22:14:53.673008Z","shell.execute_reply":"2024-05-11T22:14:53.671839Z","shell.execute_reply.started":"2024-05-11T22:13:34.167552Z"},"trusted":true},"outputs":[],"source":["if fix_pretokenizer:\n","    # Execute the shell commands\n","    subprocess.run(f\"cp -Lf models/tokenizers/{model_type}/* {outpath}/\", shell=True)\n","    \n","    # Create new tokenizer.model\n","    # subprocess.run(f\"python convert.py {outpath}/ --vocab-only --outfile {outpath}/tokenizer.model --vocab-type bpe\", shell = True)\n","\n","    subprocess.run(f\"python convert-hf-to-gguf-updated.py {outpath}/ --outtype f16 --outfile /kaggle/model/{model_name}.bin\", shell=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T22:18:10.236258Z","iopub.status.busy":"2024-05-11T22:18:10.235636Z","iopub.status.idle":"2024-05-11T22:21:07.053761Z","shell.execute_reply":"2024-05-11T22:21:07.050535Z","shell.execute_reply.started":"2024-05-11T22:18:10.236209Z"},"trusted":true},"outputs":[],"source":["#!./quantize /kaggle/model/{model_name}.bin {model_name}-q4-k-m.gguf 15    \n","import os\n","import subprocess\n","\n","# Define the model name and paths\n","original_model_path = f\"/kaggle/model/{model_name}.bin\"\n","quantized_model_path = f\"{model_name}_{quantization}.gguf\"\n","\n","# Run the quantization command\n","quantize_command = f\"./quantize {original_model_path} {quantized_model_path} {quantization}\"\n","\n","#if model_type == \"phi2\":\n","#    quantize_command = f\"python ./convert-hf-to-gguf.py {outpath}/ --outfile /kaggle/model/{model_name}.bin --outtype f16 && {quantize_command}\"\n"," \n","subprocess.run(quantize_command, shell=True)\n","\n","# Check if the quantized model exists\n","if os.path.exists(quantized_model_path):\n","    print(f\"Quantized model {quantized_model_path} exists.\")\n","\n","    # If the quantized model exists, delete the original .bin file\n","    if save_space:\n","        os.remove(original_model_path)\n","        print(f\"Deleted the original model file: {original_model_path}\")\n","else:\n","    print(f\"Quantized model {quantized_model_path} does not exist.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T20:06:45.202155Z","iopub.status.busy":"2024-05-11T20:06:45.201030Z","iopub.status.idle":"2024-05-11T20:06:45.679843Z","shell.execute_reply":"2024-05-11T20:06:45.678575Z","shell.execute_reply.started":"2024-05-11T20:06:45.202103Z"},"trusted":true},"outputs":[],"source":["from huggingface_hub import HfApi, create_repo\n","\n","api = HfApi(token=token_value)\n","repo_name = username + \"/\" + model_name + \"-\" + quantization + \"-GGUF\"\n","\n","try:\n","    # Attempt to fetch the repository details.\n","    repo_info = api.repo_info(repo_name)\n","    print(f\"Repository '{repo_name}' already exists.\")\n","    # Check if the repository is private\n","    if not repo_info.private:\n","        print(f\"Repository '{repo_name}' is public. Updating to private.\")\n","        api.update_repo_visibility(repo_id=repo_name, private=True, token=token_value)\n","        print(f\"Repository '{repo_name}' has been updated to private.\")\n","except:\n","    # If the repository does not exist, create it.\n","    create_repo(repo_name, token=token_value, private=True)\n","    print(f\"Repository '{repo_name}' has been created.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T20:30:12.663087Z","iopub.status.busy":"2024-05-11T20:30:12.662647Z","iopub.status.idle":"2024-05-11T20:30:58.907986Z","shell.execute_reply":"2024-05-11T20:30:58.906480Z","shell.execute_reply.started":"2024-05-11T20:30:12.663042Z"},"trusted":true},"outputs":[],"source":["api = HfApi(token=token_value)\n","api.upload_file(\n","    path_or_fileobj=\"/kaggle/working/llama.cpp/\"+ model_name + \"_\" + quantization + \".gguf\",\n","    path_in_repo=model_name + \"_\" + quantization + \".gguf\",\n","    repo_id=repo_name,\n","    repo_type=\"model\",\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T22:28:04.846877Z","iopub.status.busy":"2024-05-11T22:28:04.846320Z","iopub.status.idle":"2024-05-11T22:29:11.473189Z","shell.execute_reply":"2024-05-11T22:29:11.471472Z","shell.execute_reply.started":"2024-05-11T22:28:04.846835Z"},"trusted":true},"outputs":[],"source":["ggufpath = f\"/kaggle/model/{model_name}.bin\"\n","!./main -m {ggufpath} -c 512 -b 1024 -n 256 --keep 48 \\\n","    --repeat_penalty 1.0 --color \\\n","    -p \"<|im_start|>user \\\n","    I have 10 apples. I find 3 gold coins in the bottom of a river. The river runs near a big city that has something to do with what I can spend the coins on. I then lose 4 apples but gain a gold coin. Three birds run into my path and drop 6 apples each. I play an online game and win 6 gold coins but I have to share them equally with my 2 teammates. I buy apples for all the coins I have. The price of an apple is 0.5 coins. How many apples do I have? And where is the river?  \\\n","    <|im_start|>assistant\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T22:22:23.366985Z","iopub.status.busy":"2024-05-11T22:22:23.366464Z","iopub.status.idle":"2024-05-11T22:22:25.973538Z","shell.execute_reply":"2024-05-11T22:22:25.971865Z","shell.execute_reply.started":"2024-05-11T22:22:23.366945Z"},"trusted":true},"outputs":[],"source":["ggufpath = \"/kaggle/working/llama.cpp/\"+model_name+\"_\" + quantization + \".gguf\"\n","!./main -m {ggufpath} -c 512 -b 1024 -n 256 --keep 48 \\\n","    --repeat_penalty 1.0 --color \\\n","    -p \"<|im_start|>user \\\n","    I have 10 apples. I find 3 gold coins in the bottom of a river. The river runs near a big city that has something to do with what I can spend the coins on. I then lose 4 apples but gain a gold coin. Three birds run into my path and drop 6 apples each. I play an online game and win 6 gold coins but I have to share them equally with my 2 teammates. I buy apples for all the coins I have. The price of an apple is 0.5 coins. How many apples do I have? And where is the river?  \\\n","    <|im_start|>assistant\"\n","!./main -m {ggufpath} -c 512 -b 1024 -n 256 --keep 48 \\\n","    --repeat_penalty 1.0 --color \\\n","    -p \"<|im_start|>user \\\n","    Samantha has 3 brothers. Each brother has 2 sisters. How many sisters does Samantha have? \\\n","    <|im_start|>assistant\""]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
