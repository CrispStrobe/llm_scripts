{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-10T15:32:04.110850Z","iopub.status.busy":"2024-05-10T15:32:04.110435Z","iopub.status.idle":"2024-05-10T15:32:04.145131Z","shell.execute_reply":"2024-05-10T15:32:04.143973Z","shell.execute_reply.started":"2024-05-10T15:32:04.110797Z"},"trusted":true},"outputs":[],"source":["model_name='llama3-8b-spaetzle-v13' # model name that we download\n","repo_orig='cstr'  # account of the original repo from where we download the unquantized model\n","username = 'cstr' # account where we upload the quantized model to\n","fix_pretokenizer = True # must we use the pretokenizer fix with the update-script?\n","#only for the use of the convert.py script:\n","vocab_type = \"bpe\"\n","pad_vocab = True"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-10T16:20:07.808250Z","iopub.status.busy":"2024-05-10T16:20:07.807766Z","iopub.status.idle":"2024-05-10T16:25:02.291394Z","shell.execute_reply":"2024-05-10T16:25:02.289710Z","shell.execute_reply.started":"2024-05-10T16:20:07.808211Z"},"trusted":true},"outputs":[],"source":["!git clone https://github.com/ggerganov/llama.cpp\n","%cd 'llama.cpp'\n","!make"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-10T15:36:46.695185Z","iopub.status.busy":"2024-05-10T15:36:46.694854Z","iopub.status.idle":"2024-05-10T15:38:35.806678Z","shell.execute_reply":"2024-05-10T15:38:35.805054Z","shell.execute_reply.started":"2024-05-10T15:36:46.695156Z"},"trusted":true},"outputs":[],"source":["!pip install huggingface_hub\n","\n","token = 'HF_TOKEN' # you must set the token in Add-ons/Secrets and attach it to this notebook\n","\n","from kaggle_secrets import UserSecretsClient\n","user_secrets = UserSecretsClient()\n","token_value = user_secrets.get_secret(\"HF_TOKEN\")\n","\n","from huggingface_hub import snapshot_download\n","outpath = snapshot_download(repo_id=repo_orig+\"/\"+model_name, token=token_value)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-10T15:38:35.812606Z","iopub.status.busy":"2024-05-10T15:38:35.811141Z","iopub.status.idle":"2024-05-10T15:38:35.823371Z","shell.execute_reply":"2024-05-10T15:38:35.822060Z","shell.execute_reply.started":"2024-05-10T15:38:35.812548Z"},"trusted":true},"outputs":[],"source":["\n","import subprocess\n","\n","if not fix_pretokenizer:\n","    # Start command with basic parameters\n","    quantize_command = f\"python ./convert.py {outpath}/ --outtype f16 --outfile /kaggle/model/{model_name}.bin\"\n","    \n","    # Add vocab-type parameter if applicable\n","    if vocab_type != \"\":\n","        quantize_command += f\" --vocab-type {vocab_type}\"  \n","    \n","    # Add pad-vocab parameter if true\n","    if pad_vocab:\n","        quantize_command += \" --pad-vocab\"\n","    \n","    # Execute the command\n","    result = subprocess.run(quantize_command, shell=True, text=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n","\n","    # Check if the command was successful\n","    if result.returncode == 0:\n","        print(\"Command executed successfully.\")\n","        print(\"Output:\", result.stdout)\n","    else:\n","        print(\"Error in command execution.\")\n","        print(\"Error:\", result.stderr)\n","\n","#!python ./convert.py {outpath}/ --outtype f16 --outfile /kaggle/model/{model_name}.bin --vocab-type bpe --pad-vocab\n","#!python ./convert.py {outpath}/ --outtype f16 --outfile /kaggle/model/{model_name}.bin --pad-vocab"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-10T16:31:58.533111Z","iopub.status.busy":"2024-05-10T16:31:58.532572Z","iopub.status.idle":"2024-05-10T16:31:58.550493Z","shell.execute_reply":"2024-05-10T16:31:58.549225Z","shell.execute_reply.started":"2024-05-10T16:31:58.533072Z"},"trusted":true},"outputs":[],"source":["%%writefile \"convert-hf-to-gguf-update.py\"\n","#!/usr/bin/env python3\n","\n","# This script downloads the tokenizer models of the specified models from Huggingface and\n","# generates the get_vocab_base_pre() function for convert-hf-to-gguf.py\n","#\n","# This is necessary in order to analyze the type of pre-tokenizer used by the model and\n","# provide the necessary information to llama.cpp via the GGUF header in order to implement\n","# the same pre-tokenizer.\n","#\n","# ref: https://github.com/ggerganov/llama.cpp/pull/6920\n","#\n","# Instructions:\n","#\n","# - Add a new model to the \"models\" list\n","# - Run the script with your huggingface token:\n","#\n","#   python3 convert-hf-to-gguf-update.py <huggingface_token>\n","#\n","# - Copy-paste the generated get_vocab_base_pre() function into convert-hf-to-gguf.py\n","# - Update llama.cpp with the new pre-tokenizer if necessary\n","#\n","# TODO: generate tokenizer tests for llama.cpp\n","# TODO: automate the update of convert-hf-to-gguf.py\n","#\n","\n","import logging\n","import os\n","import requests\n","import sys\n","import json\n","\n","from hashlib import sha256\n","from enum import IntEnum, auto\n","from transformers import AutoTokenizer\n","\n","logging.basicConfig(level=logging.DEBUG)\n","logger = logging.getLogger(\"convert-hf-to-gguf-update\")\n","\n","\n","class TOKENIZER_TYPE(IntEnum):\n","    SPM = auto()\n","    BPE = auto()\n","    WPM = auto()\n","\n","\n","# TODO: this string has to exercise as much pre-tokenizer functionality as possible\n","#       will be updated with time - contributions welcome\n","chktxt = '\\n \\n\\n \\n\\n\\n \\t \\t\\t \\t\\n  \\n   \\n    \\n     \\nüöÄ (normal) üò∂‚Äçüå´Ô∏è (multiple emojis concatenated) ‚úÖ ü¶ôü¶ô 3 33 333 3333 33333 333333 3333333 33333333 3.3 3..3 3...3 ·ûÄ·û∂·ûì·üã·ûè·üÇ·ûñ·û∑·ûü·üÅ·ûü·û¢·û∂·ûÖüòÅ ?ÊàëÊÉ≥Âú®appleÂ∑•‰Ωú1314151Â§©ÔΩû ------======= –Ω–µ—â–æ –Ω–∞ –ë—ä–ª–≥–∞—Ä—Å–∫–∏ \\'\\'\\'\\'\\'\\'```````\\\"\\\"\\\"\\\"......!!!!!!?????? I\\'ve been \\'told he\\'s there, \\'RE you sure? \\'M not sure I\\'ll make it, \\'D you like some tea? We\\'Ve a\\'lL'\n","\n","if len(sys.argv) == 2:\n","    token = sys.argv[1]\n","else:\n","    logger.info(\"Usage: python convert-hf-to-gguf-update.py <huggingface_token>\")\n","    sys.exit(1)\n","\n","# TODO: add models here, base models preferred\n","models = [\n","    {\"name\": \"llama-bpe\",      \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/meta-llama/Meta-Llama-3-8B\", },\n","    {\"name\": \"phi-3\",          \"tokt\": TOKENIZER_TYPE.SPM, \"repo\": \"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\", },\n","    {\"name\": \"deepseek-llm\",   \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/deepseek-ai/deepseek-llm-7b-base\", },\n","    {\"name\": \"deepseek-coder\", \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-base\", },\n","    {\"name\": \"falcon\",         \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/tiiuae/falcon-7b\", },\n","    {\"name\": \"bert-bge\",       \"tokt\": TOKENIZER_TYPE.WPM, \"repo\": \"https://huggingface.co/BAAI/bge-small-en-v1.5\", },\n","    {\"name\": \"mpt\",            \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/mosaicml/mpt-7b\", },\n","    {\"name\": \"starcoder\",      \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/bigcode/starcoder2-3b\", },\n","    {\"name\": \"gpt-2\",          \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/openai-community/gpt2\", },\n","    {\"name\": \"refact\",         \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/smallcloudai/Refact-1_6-base\", },\n","    #{\"name\": \"command-r\",      \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/CohereForAI/c4ai-command-r-v01\", },\n","]\n","\n","# make directory \"models/tokenizers\" if it doesn't exist\n","if not os.path.exists(\"models/tokenizers\"):\n","    os.makedirs(\"models/tokenizers\")\n","\n","\n","def download_file_with_auth(url, token, save_path):\n","    headers = {\"Authorization\": f\"Bearer {token}\"}\n","    response = requests.get(url, headers=headers)\n","    if response.status_code == 200:\n","        with open(save_path, 'wb') as f:\n","            f.write(response.content)\n","        logger.info(f\"File {save_path} downloaded successfully\")\n","    else:\n","        logger.info(f\"Failed to download file. Status code: {response.status_code}\")\n","\n","\n","# download the tokenizer models\n","for model in models:\n","    name = model[\"name\"]\n","    repo = model[\"repo\"]\n","    tokt = model[\"tokt\"]\n","\n","    if not os.path.exists(f\"models/tokenizers/{name}\"):\n","        os.makedirs(f\"models/tokenizers/{name}\")\n","    else:\n","        logger.info(f\"Directory models/tokenizers/{name} already exists - skipping\")\n","        continue\n","\n","    logger.info(f\"Downloading {name} to models/tokenizers/{name}\")\n","\n","    url = f\"{repo}/raw/main/config.json\"\n","    save_path = f\"models/tokenizers/{name}/config.json\"\n","    download_file_with_auth(url, token, save_path)\n","\n","    url = f\"{repo}/raw/main/tokenizer.json\"\n","    save_path = f\"models/tokenizers/{name}/tokenizer.json\"\n","    download_file_with_auth(url, token, save_path)\n","\n","    # if downloaded file is less than 1KB, we likely need to download an LFS instead\n","    if os.path.getsize(save_path) < 1024:\n","        # remove the file\n","        os.remove(save_path)\n","        url = f\"{repo}/resolve/main/tokenizer.json\"\n","        save_path = f\"models/tokenizers/{name}/tokenizer.json\"\n","        download_file_with_auth(url, token, save_path)\n","\n","    if tokt == TOKENIZER_TYPE.SPM:\n","        url = f\"{repo}/resolve/main/tokenizer.model\"\n","        save_path = f\"models/tokenizers/{name}/tokenizer.model\"\n","        download_file_with_auth(url, token, save_path)\n","\n","    url = f\"{repo}/raw/main/tokenizer_config.json\"\n","    save_path = f\"models/tokenizers/{name}/tokenizer_config.json\"\n","    download_file_with_auth(url, token, save_path)\n","\n","# generate the source code for the convert-hf-to-gguf.py:get_vocab_base_pre() function:\n","# TODO: auto-update convert-hf-to-gguf.py with the generated function\n","\n","src_ifs = \"\"\n","for model in models:\n","    name = model[\"name\"]\n","    tokt = model[\"tokt\"]\n","\n","    if tokt == TOKENIZER_TYPE.SPM:\n","        continue\n","\n","    # create the tokenizer\n","    tokenizer = AutoTokenizer.from_pretrained(f\"models/tokenizers/{name}\")\n","\n","    chktok = tokenizer.encode(chktxt)\n","    chkhsh = sha256(str(chktok).encode()).hexdigest()\n","\n","    logger.info(f\"model: {name}\")\n","    logger.info(f\"tokt: {tokt}\")\n","    logger.info(f\"repo: {model['repo']}\")\n","    logger.info(f\"chktok: {chktok}\")\n","    logger.info(f\"chkhsh: {chkhsh}\")\n","\n","    # print the \"pre_tokenizer\" content from the tokenizer.json\n","    with open(f\"models/tokenizers/{name}/tokenizer.json\", \"r\", encoding=\"utf-8\") as f:\n","        cfg = json.load(f)\n","        pre_tokenizer = cfg[\"pre_tokenizer\"]\n","        logger.info(\"pre_tokenizer: \" + json.dumps(pre_tokenizer, indent=4))\n","\n","    logger.info(\"\")\n","\n","    src_ifs += f\"        if chkhsh == \\\"{chkhsh}\\\":\\n\"\n","    src_ifs += f\"            # ref: {model['repo']}\\n\"\n","    src_ifs += f\"            res = \\\"{name}\\\"\\n\"\n","\n","src_func = f\"\"\"\n","    def get_vocab_base_pre(self, tokenizer) -> str:\n","        # encoding this string and hashing the resulting tokens would (hopefully) give us a unique identifier that\n","        # is specific for the BPE pre-tokenizer used by the model\n","        # we will use this unique identifier to write a \"tokenizer.ggml.pre\" entry in the GGUF file which we can\n","        # use in llama.cpp to implement the same pre-tokenizer\n","\n","        chktxt = {repr(chktxt)}\n","\n","        chktok = tokenizer.encode(chktxt)\n","        chkhsh = sha256(str(chktok).encode()).hexdigest()\n","\n","        logger.debug(f\"chktok: {{chktok}}\")\n","        logger.debug(f\"chkhsh: {{chkhsh}}\")\n","\n","        res = None\n","\n","        # NOTE: if you get an error here, you need to update the convert-hf-to-gguf-update.py script\n","        #       or pull the latest version of the model from Huggingface\n","        #       don't edit the hashes manually!\n","{src_ifs}\n","        if res is None:\n","            logger.warning(\"\\\\n\")\n","            logger.warning(\"**************************************************************************************\")\n","            logger.warning(\"** WARNING: The BPE pre-tokenizer was not recognized!\")\n","            logger.warning(\"**          There are 2 possible reasons for this:\")\n","            logger.warning(\"**          - the model has not been added to convert-hf-to-gguf-update.py yet\")\n","            logger.warning(\"**          - the pre-tokenization config has changed upstream\")\n","            logger.warning(\"**          Check your model files and convert-hf-to-gguf-update.py and update them accordingly.\")\n","            logger.warning(\"** ref:     https://github.com/ggerganov/llama.cpp/pull/6920\")\n","            logger.warning(\"**\")\n","            logger.warning(f\"** chkhsh:  {{chkhsh}}\")\n","            logger.warning(\"**************************************************************************************\")\n","            logger.warning(\"\\\\n\")\n","            raise NotImplementedError(\"BPE pre-tokenizer was not recognized - update get_vocab_base_pre()\")\n","\n","        logger.debug(f\"tokenizer.ggml.pre: {{repr(res)}}\")\n","        logger.debug(f\"chkhsh: {{chkhsh}}\")\n","\n","        return res\n","\"\"\"\n","\n","print(src_func) # noqa: NP100\n","\n","logger.info(\"\\n\")\n","logger.info(\"!!! Copy-paste the function above into convert-hf-to-gguf.py !!!\")\n","logger.info(\"\\n\")\n","\n","# generate tests for each tokenizer model\n","\n","tests = [\n","    \"ied 4 ¬Ω months\",\n","    \"F√ºhrer\",\n","    \"\",\n","    \" \",\n","    \"  \",\n","    \"   \",\n","    \"\\t\",\n","    \"\\n\",\n","    \"\\n\\n\",\n","    \"\\n\\n\\n\",\n","    \"\\t\\n\",\n","    \"Hello world\",\n","    \" Hello world\",\n","    \"Hello World\",\n","    \" Hello World\",\n","    \" Hello World!\",\n","    \"Hello, world!\",\n","    \" Hello, world!\",\n","    \" this is ü¶ô.cpp\",\n","    \"w048 7tuijk dsdfhu\",\n","    \"–Ω–µ—â–æ –Ω–∞ –ë—ä–ª–≥–∞—Ä—Å–∫–∏\",\n","    \"·ûÄ·û∂·ûì·üã·ûè·üÇ·ûñ·û∑·ûü·üÅ·ûü·û¢·û∂·ûÖ·ûÅ·ûõ·ûÖ·üÅ·ûâ\",\n","    \"üöÄ (normal) üò∂‚Äçüå´Ô∏è (multiple emojis concatenated) ‚úÖ (only emoji that has its own token)\",\n","    \"Hello\",\n","    \" Hello\",\n","    \"  Hello\",\n","    \"   Hello\",\n","    \"    Hello\",\n","    \"    Hello\\n    Hello\",\n","    \" (\",\n","    \"\\n =\",\n","    \"' era\",\n","    \"Hello, y'all! How are you üòÅ ?ÊàëÊÉ≥Âú®appleÂ∑•‰Ωú1314151Â§©ÔΩû\",\n","    \"3\",\n","    \"33\",\n","    \"333\",\n","    \"3333\",\n","    \"33333\",\n","    \"333333\",\n","    \"3333333\",\n","    \"33333333\",\n","    \"333333333\",\n","    chktxt,\n","]\n","\n","# write the tests to ./models/ggml-vocab-{name}.gguf.inp\n","# the format is:\n","#\n","# test0\n","# __ggml_vocab_test__\n","# test1\n","# __ggml_vocab_test__\n","# ...\n","#\n","\n","# with each model, encode all tests and write the results in ./models/ggml-vocab-{name}.gguf.out\n","# for each test, write the resulting tokens on a separate line\n","\n","for model in models:\n","    name = model[\"name\"]\n","    tokt = model[\"tokt\"]\n","\n","    # create the tokenizer\n","    tokenizer = AutoTokenizer.from_pretrained(f\"models/tokenizers/{name}\")\n","\n","    with open(f\"models/ggml-vocab-{name}.gguf.inp\", \"w\", encoding=\"utf-8\") as f:\n","        for text in tests:\n","            f.write(f\"{text}\")\n","            f.write(\"\\n__ggml_vocab_test__\\n\")\n","\n","    with open(f\"models/ggml-vocab-{name}.gguf.out\", \"w\") as f:\n","        for text in tests:\n","            res = tokenizer.encode(text, add_special_tokens=False)\n","            for r in res:\n","                f.write(f\" {r}\")\n","            f.write(\"\\n\")\n","\n","    logger.info(f\"Tests for {name} written in ./models/ggml-vocab-{name}.gguf.*\")\n","\n","# generate commands for creating vocab files\n","\n","logger.info(\"\\nRun the following commands to generate the vocab files for testing:\\n\")\n","\n","for model in models:\n","    name = model[\"name\"]\n","\n","    print(f\"python3 convert-hf-to-gguf.py models/tokenizers/{name}/ --outfile models/ggml-vocab-{name}.gguf --vocab-only\") # noqa: NP100\n","\n","logger.info(\"\\n\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-10T16:32:11.708333Z","iopub.status.busy":"2024-05-10T16:32:11.707855Z","iopub.status.idle":"2024-05-10T16:32:11.719744Z","shell.execute_reply":"2024-05-10T16:32:11.718488Z","shell.execute_reply.started":"2024-05-10T16:32:11.708299Z"},"trusted":true},"outputs":[],"source":["import subprocess\n","\n","if fix_pretokenizer:\n","    # Read and modify the existing script\n","    with open(\"convert-hf-to-gguf-update.py\", \"r\") as file:\n","        script_contents = file.read()\n","\n","    new_repo_name = repo_orig + \"/\" + model_name\n","    \n","    # Define the new line to be inserted\n","    new_line = f'    {{\"name\": \"llama-bpe-1\", \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/{new_repo_name}\", }},'\n","    marker_line = '{\"name\": \"llama-bpe\",      \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/meta-llama/Meta-Llama-3-8B\", },'\n","    # Insert the new line after the marker line\n","    marker_found = False\n","    lines = script_contents.split('\\n')\n","    for i, line in enumerate(lines):\n","        if marker_line in line:\n","            lines.insert(i + 1, new_line)\n","            marker_found = True\n","            break\n","\n","    if not marker_found:\n","        print(\"Marker line not found in the file.\")\n","\n","    else:\n","        updated_script = \"\\n\".join(lines)\n","        # Write the updated script to the file\n","        with open(\"convert-hf-to-gguf-update.py\", \"w\") as file:\n","            file.write(updated_script)\n","        print(\"Script updated successfully.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-10T16:32:16.343710Z","iopub.status.busy":"2024-05-10T16:32:16.342826Z","iopub.status.idle":"2024-05-10T16:32:26.226964Z","shell.execute_reply":"2024-05-10T16:32:26.225681Z","shell.execute_reply.started":"2024-05-10T16:32:16.343665Z"},"trusted":true},"outputs":[],"source":["if fix_pretokenizer:\n","    # Execute the script and capture output\n","    command = f\"python convert-hf-to-gguf-update.py {token_value} > output.txt\"\n","    subprocess.run(command, shell=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-10T16:41:57.377449Z","iopub.status.busy":"2024-05-10T16:41:57.376912Z","iopub.status.idle":"2024-05-10T16:41:57.393141Z","shell.execute_reply":"2024-05-10T16:41:57.391896Z","shell.execute_reply.started":"2024-05-10T16:41:57.377410Z"},"trusted":true},"outputs":[],"source":["if fix_pretokenizer:\n","    # Read the output from the file and trim it after a specific line\n","    end_line = \"return res\"\n","    with open('output.txt', 'r') as file:\n","        lines = file.readlines()\n","        trimmed_output = []\n","        for line in lines:\n","            trimmed_output.append(line)\n","            if end_line in line:\n","                break  # Stop adding lines after the end_line is found\n","    \n","    # Join the trimmed lines back into a single string\n","    script_output = \"\".join(trimmed_output)\n","    print(\"Captured Output:\", script_output)\n","\n","    # Function replacement from update script output\n","    def replace_function(source_path, output_path, start_marker, end_marker, new_function_content):\n","        with open(source_path, 'r') as source_file, open(output_path, 'w') as output_file:\n","            in_old_function = False\n","            for line in source_file:\n","                if start_marker in line:\n","                    in_old_function = True\n","                    output_file.write(new_function_content + \"\\n\")\n","                    continue\n","                if in_old_function and end_marker in line:\n","                    in_old_function = False\n","                    continue\n","                if not in_old_function:\n","                    output_file.write(line)\n","\n","    # Setup of the function replacement\n","    start_marker = \"def get_vocab_base_pre(self, tokenizer) -> str:\"\n","    end_marker = \"return res\"\n","    new_function_content = script_output  # Use the trimmed script output as the new content\n","\n","    source_path = \"convert-hf-to-gguf.py\"\n","    output_path = \"convert-hf-to-gguf-updated.py\"\n","    replace_function(source_path, output_path, start_marker, end_marker, new_function_content)\n","    print(\"Function replacement complete.\")\n"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2024-05-10T16:43:20.802563Z","iopub.status.busy":"2024-05-10T16:43:20.802070Z","iopub.status.idle":"2024-05-10T16:43:21.903291Z","shell.execute_reply":"2024-05-10T16:43:21.901689Z","shell.execute_reply.started":"2024-05-10T16:43:20.802528Z"},"trusted":true},"outputs":[],"source":["!mkdir /kaggle/model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-10T16:54:31.025604Z","iopub.status.busy":"2024-05-10T16:54:31.025078Z","iopub.status.idle":"2024-05-10T16:54:35.142083Z","shell.execute_reply":"2024-05-10T16:54:35.140716Z","shell.execute_reply.started":"2024-05-10T16:54:31.025562Z"},"trusted":true},"outputs":[],"source":["#subprocess.run(f\"python convert.py {outpath}/ --vocab-only --outfile {outpath}/tokenizer.model --vocab-type bpe\", shell = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-10T16:45:52.354554Z","iopub.status.busy":"2024-05-10T16:45:52.354021Z","iopub.status.idle":"2024-05-10T16:47:03.850262Z","shell.execute_reply":"2024-05-10T16:47:03.848694Z","shell.execute_reply.started":"2024-05-10T16:45:52.354511Z"},"trusted":true},"outputs":[],"source":["#!python3 convert-hf-to-gguf.py models/tokenizers/llama-bpe-1/ --outfile models/ggml-vocab-llama-bpe-1.gguf --vocab-only"]},{"cell_type":"code","execution_count":70,"metadata":{"execution":{"iopub.execute_input":"2024-05-10T16:55:41.219532Z","iopub.status.busy":"2024-05-10T16:55:41.219080Z","iopub.status.idle":"2024-05-10T16:55:42.341150Z","shell.execute_reply":"2024-05-10T16:55:42.339647Z","shell.execute_reply.started":"2024-05-10T16:55:41.219495Z"},"trusted":true},"outputs":[],"source":["!rm {outpath}/tokenizer.model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-10T16:55:46.309905Z","iopub.status.busy":"2024-05-10T16:55:46.308890Z"},"trusted":true},"outputs":[],"source":["if fix_pretokenizer:\n","    # Execute the shell commands\n","    subprocess.run(f\"cp -Lf models/tokenizers/llama-bpe-1/* {outpath}/\", shell=True)\n","    \n","    # Create new tokenizer.model\n","    # subprocess.run(f\"python convert.py {outpath}/ --vocab-only --outfile {outpath}/tokenizer.model --vocab-type bpe\", shell = True)\n","\n","    subprocess.run(f\"python convert-hf-to-gguf-updated.py {outpath}/ --outtype f16 --outfile /kaggle/model/{model_name}.bin\", shell=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-10T15:38:37.927264Z","iopub.status.idle":"2024-05-10T15:38:37.927994Z","shell.execute_reply":"2024-05-10T15:38:37.927647Z","shell.execute_reply.started":"2024-05-10T15:38:37.927625Z"},"trusted":true},"outputs":[],"source":["#!./quantize /kaggle/model/{model_name}.bin {model_name}-q4-k-m.gguf 15    \n","import os\n","import subprocess\n","\n","# Define the model name and paths\n","original_model_path = f\"/kaggle/model/{model_name}.bin\"\n","quantized_model_path = f\"{model_name}-q4-k-m.gguf\"\n","\n","# Run the quantization command\n","quantize_command = f\"./quantize {original_model_path} {quantized_model_path} 15\"\n","subprocess.run(quantize_command, shell=True)\n","\n","# Check if the quantized model exists\n","if os.path.exists(quantized_model_path):\n","    print(f\"Quantized model {quantized_model_path} exists.\")\n","\n","    # If the quantized model exists, delete the original .bin file\n","    os.remove(original_model_path)\n","    print(f\"Deleted the original model file: {original_model_path}\")\n","else:\n","    print(f\"Quantized model {quantized_model_path} does not exist.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-10T15:38:37.930272Z","iopub.status.idle":"2024-05-10T15:38:37.930700Z","shell.execute_reply":"2024-05-10T15:38:37.930516Z","shell.execute_reply.started":"2024-05-10T15:38:37.930498Z"},"trusted":true},"outputs":[],"source":["from huggingface_hub import HfApi, create_repo\n","\n","api = HfApi(token=token_value)\n","repo_name = username + \"/\" + model_name + \"-GGUF\"\n","\n","try:\n","    # Attempt to fetch the repository details.\n","    repo_info = api.repo_info(repo_name)\n","    print(f\"Repository '{repo_name}' already exists.\")\n","    # Check if the repository is private\n","    if not repo_info.private:\n","        print(f\"Repository '{repo_name}' is public. Updating to private.\")\n","        api.update_repo_visibility(repo_id=repo_name, private=True, token=token_value)\n","        print(f\"Repository '{repo_name}' has been updated to private.\")\n","except:\n","    # If the repository does not exist, create it.\n","    create_repo(repo_name, token=token_value, private=True)\n","    print(f\"Repository '{repo_name}' has been created.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-10T15:38:37.933292Z","iopub.status.idle":"2024-05-10T15:38:37.933877Z","shell.execute_reply":"2024-05-10T15:38:37.933595Z","shell.execute_reply.started":"2024-05-10T15:38:37.933571Z"},"trusted":true},"outputs":[],"source":["api = HfApi(token=token_value)\n","api.upload_file(\n","    path_or_fileobj=\"/kaggle/working/llama.cpp/\"+model_name+\"-q4-k-m.gguf\",\n","    path_in_repo=model_name+\"-q4-k-m.gguf\",\n","    repo_id=repo_name,\n","    repo_type=\"model\",\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-10T15:38:37.935091Z","iopub.status.idle":"2024-05-10T15:38:37.936049Z","shell.execute_reply":"2024-05-10T15:38:37.935736Z","shell.execute_reply.started":"2024-05-10T15:38:37.935698Z"},"trusted":true},"outputs":[],"source":["ggufpath = \"/kaggle/working/llama.cpp/\"+model_name+\"-q4-k-m.gguf\"\n","!./main -m {ggufpath} -c 512 -b 1024 -n 256 --keep 48 \\\n","    --repeat_penalty 1.0 --color \\\n","    -p \"<|im_start|>user \\\n","    I have 10 apples. I find 3 gold coins in the bottom of a river. The river runs near a big city that has something to do with what I can spend the coins on. I then lose 4 apples but gain a gold coin. Three birds run into my path and drop 6 apples each. I play an online game and win 6 gold coins but I have to share them equally with my 2 teammates. I buy apples for all the coins I have. The price of an apple is 0.5 coins. How many apples do I have? And where is the river?  \\\n","    <|im_start|>assistant\"\n","!./main -m {ggufpath} -c 512 -b 1024 -n 256 --keep 48 \\\n","    --repeat_penalty 1.0 --color \\\n","    -p \"<|im_start|>user \\\n","    Samantha has 3 brothers. Each brother has 2 sisters. How many sisters does Samantha have? \\\n","    <|im_start|>assistant\""]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
